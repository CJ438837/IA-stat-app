import streamlit as st
import pandas as pd
import re
from Bio import Entrez
import numpy as np

# === Configuration ===
Entrez.email = "ton.email@example.com"

st.title("üß† Analyse statistique automatis√©e")
st.write("Importe ton fichier Excel pour g√©n√©rer une analyse descriptive et des visualisations automatiquement.")

# --- T√©l√©versement de fichier ---
uploaded_file = st.file_uploader("üìÇ Importer un fichier Excel (.xlsx)", type=["xlsx"])

if uploaded_file is not None:
    # Lecture du fichier
    data_dict = pd.read_excel(uploaded_file, sheet_name=None)
    st.success(f"‚úÖ Fichier import√© : {uploaded_file.name}")
    
    # S√©lection de la premi√®re feuille
    sheet_name = list(data_dict.keys())[0]
    df_sheet = data_dict[sheet_name]

    # --- Description de l‚Äô√©tude ---
    description = st.text_area("D√©cris ton √©tude en quelques phrases :", "")
    
    if st.button("Analyser"):
        if not description.strip():
            st.warning("Merci de d√©crire bri√®vement ton √©tude avant de lancer l'analyse.")
        else:
            # --- Extraction des mots-cl√©s ---
            tokens = re.findall(r'\b\w+\b', description.lower())
            stopwords_fr = set([
                "le","la","les","un","une","des","de","du","et","en","au","aux","avec",
                "pour","sur","dans","par","au","a","ce","ces","est","sont","ou","o√π",
                "se","sa","son","que","qui","ne","pas","plus","moins","comme","donc"
            ])
            keywords_fr = [w for w in tokens if w not in stopwords_fr]

            st.markdown("### üß© Mots-cl√©s extraits")
            st.write(f"**Fran√ßais :** {keywords_fr}")

            # --- Import des fonctions IA-Stat ---
            from IA_STAT_descriptive_251125 import descriptive_analysis
            from IA_STAT_distribution_251125 import advanced_distribution_analysis
            from IA_STAT_interactif2 import propose_tests_interactif

            # --- Fonction adapt√©e pour DataFrame d√©j√† charg√© ---
            def detect_variable_types_df(df):
                results = []
                for col in df.columns:
                    col_data = df[col].dropna()
                    if col_data.empty:
                        continue

                    unique_vals = pd.Series(col_data).astype(str).str.strip().unique()
                    n_unique = len(unique_vals)

                    if n_unique == 2:
                        var_type = "binaire"
                    elif np.issubdtype(col_data.dtype, np.number):
                        var_type = "num√©rique"
                    else:
                        var_type = "cat√©gorielle"

                    results.append({
                        "variable": col,
                        "type": var_type,
                        "valeurs_uniques": n_unique,
                        "exemples": unique_vals[:5]
                    })

                types_df = pd.DataFrame(results)
                return {"data": types_df}, {"data": df}

            # --- D√©tection des types ---
            types_dict, data_dict_adapted = detect_variable_types_df(df_sheet)
            types_df = types_dict['data']
            df_sheet = data_dict_adapted['data']

            # --- Analyse descriptive ---
            summary = descriptive_analysis(df_sheet, types_df)
            st.markdown("### üìä Analyse descriptive")
            for var, stats in summary.items():
                st.write(f"**{var}** :")
                st.json(stats)

            # --- Analyse de distribution avanc√©e ---
            st.markdown("### üìà Analyse de distribution")
            distribution_df = advanced_distribution_analysis(df_sheet, types_df, output_folder="./plots")
            st.dataframe(distribution_df)

            # --- Tests statistiques interactifs ---
            st.markdown("### üßÆ Tests statistiques sugg√©r√©s")
            # On passe directement les mots-cl√©s fran√ßais
            propose_tests_interactif(types_df, distribution_df, df_sheet, keywords_fr)

else:
    st.info("üí° Importez un fichier Excel pour commencer.")
